import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList
import torch
from collections import deque

# ëª¨ë¸ ë¡œë“œ
st.title("LLaMA 3 Chatbot ğŸ¤–")
st.sidebar.header("Settings")

model_id = "meta-llama/Llama-3.2-3B-Instruct"
max_tokens = st.sidebar.slider("Max Tokens", min_value=50, max_value=500, value=300, step=50)

@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto",
        pad_token_id=tokenizer.eos_token_id
    )
    model = torch.compile(model)
    return tokenizer, model

tokenizer, model = load_model()

# ë¬¸ì¥ì´ ëë‚  ë•Œê¹Œì§€ ì‘ë‹µ ìœ ì§€
class StopOnSentenceEnd(StoppingCriteria):
    def __init__(self, min_length):
        self.min_length = min_length

    def __call__(self, input_ids, scores, **kwargs):
        if input_ids.shape[1] < self.min_length:
            return False  # ìµœì†Œ ê¸¸ì´ ë„ë‹¬ ì „ì—ëŠ” ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
        text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
        return text.strip().endswith(('.', '!', '?'))  # ë¬¸ì¥ì´ ì™„ì „íˆ ëë‚  ë•Œë§Œ ì¤‘ë‹¨

def generate_response(user_input, max_tokens=300):
    """
    ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    prompt = f"User: {user_input}"  # Botì„ ë¯¸ë¦¬ ì¶”ê°€í•˜ì§€ ì•Šê³ , ì‚¬ìš©ì ì…ë ¥ë§Œ ì‚¬ìš©

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    stopping_criteria = StoppingCriteriaList([StopOnSentenceEnd(min_length=max_tokens // 2)])

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_length=int(max_tokens * 1.2),
            do_sample=True,
            top_k=50,
            top_p=0.9,
            temperature=0.7,
            num_return_sequences=1,
            repetition_penalty=1.2,
            no_repeat_ngram_size=2,
            pad_token_id=tokenizer.eos_token_id,
            early_stopping=False,
            stopping_criteria=stopping_criteria
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()

    # "Bot:"ì´ ìë™ í¬í•¨ë˜ë©´ ì œê±°
    if response.startswith("Bot:"):
        response = response[len("Bot:"):].strip()

    return response


def generate_full_response(user_input, max_tokens=300, max_turns=1):
    """
    ëª¨ë¸ì´ ì¤‘ê°„ì— ì˜ë¦¬ëŠ” ê²½ìš°, ì¶”ê°€ì ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """
    response_list = []  # ì—¬ëŸ¬ ì‘ë‹µì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    for _ in range(max_turns):
        new_text = generate_response(user_input, max_tokens)
        
        # ì‘ë‹µì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¶”ê°€ ìƒì„±
        if len(new_text.split()) < max_tokens // 3:
            user_input = new_text  # ìƒˆ ì‘ë‹µì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì„¤ì •
            continue
        
        response_list.append(new_text)
        if new_text.strip().endswith(('.', '!', '?')):  # ë¬¸ì¥ì´ ì™„ì „íˆ ëë‚¬ë‹¤ë©´ ì¤‘ë‹¨
            break

    return " ".join(response_list).strip()  # ëª¨ë“  ì‘ë‹µì„ í•©ì³ ìµœì¢… ê²°ê³¼ ë°˜í™˜



def chat_with_memory(history, user_input, max_tokens=150):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•˜ê³ , ëŒ€í™” ì´ë ¥ì„ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    response = generate_response(user_input, max_tokens)

    # ì¤‘ë³µëœ ì‘ë‹µì„ ë°©ì§€í•˜ê³  ì •ë¦¬í•˜ì—¬ ì¶”ê°€
    if not history or history[-1] != f"User: {user_input}":
        history.append(f"User: {user_input}")
    
    if not history or history[-1] != f"Bot: {response}":
        history.append(f"Bot: {response}")

    return response, history, max_tokens


# Streamlit ì¸í„°í˜ì´ìŠ¤
st.subheader("Ask Something To Bot! ")

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "history" not in st.session_state:
    st.session_state.history = deque(maxlen=10)  # ìµœê·¼ 10ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€

# ì‚¬ìš©ì ì…ë ¥
user_input = st.text_input("You:", key="user_input")

if st.button("Send"):
    if user_input:
        response, updated_history, max_tokens = chat_with_memory(st.session_state.history, user_input, max_tokens)

st.subheader("Chat History")
for message in st.session_state.history:
    st.write(message)
