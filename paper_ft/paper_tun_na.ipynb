{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkwon/.local/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/kkwon/.local/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDFÏóêÏÑú textÎ°ú Î≥ÄÌôòÌïòÎäî Ìï®Ïàò\n",
    "+ txt To csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in PDF: 6\n",
      "Page 1: 6171 characters extracted\n",
      "Page 2: 5578 characters extracted\n",
      "Page 3: 4214 characters extracted\n",
      "Page 4: 6023 characters extracted\n",
      "Page 5: 2765 characters extracted\n",
      "Page 6: 5628 characters extracted\n",
      "Text extraction complete. Total characters extracted: 29507\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "def pdf_to_text(pdf_path, skip_start_pages=0, skip_last_pages=0, header_lines=1, footer_lines=1):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"Total pages in PDF: {num_pages}\")\n",
    "\n",
    "        # Adjust the range to process the correct pages\n",
    "        start_page = skip_start_pages\n",
    "        end_page = num_pages - skip_last_pages\n",
    "\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            if page_text:\n",
    "                print(f\"Page {page_num + 1}: {len(page_text)} characters extracted\")\n",
    "                lines = page_text.splitlines(True)[header_lines:-footer_lines]\n",
    "                text += \"\".join(lines)\n",
    "            else:\n",
    "                print(f\"Page {page_num + 1} is empty or could not be read\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "# Define the input and output file paths\n",
    "pdf_file_path = \"/home/kkwon/AHN/paper_ft/datas/3362743.3362963.pdf\"\n",
    "output_file_path = \"/home/kkwon/AHN/paper_ft/datas/paper1.txt\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "# Adjust skip_start_pages and skip_last_pages as needed\n",
    "raw_text = pdf_to_text(pdf_file_path, skip_start_pages=0, skip_last_pages=0, header_lines=2, footer_lines=1)\n",
    "\n",
    "# Save the extracted text to a text file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(raw_text)\n",
    "\n",
    "print(f\"Text extraction complete. Total characters extracted: {len(raw_text)}\")\n",
    "\n",
    "    \n",
    "# data=re.sub(r'[\\n\\t\\r]',' ',raw_text)\n",
    "# sentences=re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s',data)\n",
    "# sentences=[sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "# unique_sentences=list(dict.fromkeys(sentences))\n",
    "\n",
    "# df=pd.DataFrame(unique_sentences,columns=['Text'])\n",
    "# df.to_csv('/home/kkwon/AHN/paper_ft/cleaned_paper.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model / Dataset ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdf6fdc2c594264b0aca0fbd23248ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkwon/.local/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m=\u001b[39mget_peft_model(model,peft_config)\n\u001b[1;32m     49\u001b[0m training_args\u001b[38;5;241m=\u001b[39mTrainingArguments(\n\u001b[1;32m     50\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mnew_model,\n\u001b[1;32m     51\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m#report_to=\"tensorboard\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     76\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     77\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     78\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \n\u001b[0;32m---> 79\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     80\u001b[0m     \n\u001b[1;32m     81\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(\n\u001b[1;32m     82\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     83\u001b[0m         mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m     ),\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_DISABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/dataset_dict.py:75\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     78\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     79\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "model_id=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "new_model=\"Llama-3.2-3B-papers\"\n",
    "\n",
    "\n",
    "dataset = load_dataset('csv', data_files='/home/kkwon/AHN/paper_ft/cleaned_paper.csv')\n",
    "\n",
    "torch_dtype=torch.float16\n",
    "attn_implementation='eager'\n",
    "\n",
    "#QLoRA config\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "#Load model\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "#Load tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "\n",
    "#LoRA config\n",
    "peft_config=LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    bias='none',\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj','down_proj','gate_proj','k_proj','q_proj','v_proj','o_proj']\n",
    ")\n",
    "model=get_peft_model(model,peft_config)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Text'],padding='max_length',truncation=True)\n",
    "\n",
    "tokenized_dataset=dataset.map(tokenize_function,batched=True)\n",
    "\n",
    "model=get_peft_model(model,peft_config)\n",
    "\n",
    "training_args=TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    #weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    #max_grad_norm=0.3,\n",
    "    #max_steps=-1,\n",
    "    #warmup_ratio=0.03,\n",
    "    group_by_length=True\n",
    "    #lr_scheduler_type=\"constant\",\n",
    "    #report_to=\"tensorboard\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],  \n",
    "    eval_dataset=tokenized_dataset[\"eval\"],\n",
    "    \n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    ),\n",
    ")\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkwon/.local/lib/python3.8/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67879021-139b66fb38047eda4cde2952;fe08a308-5aa2-4291-952f-9e524bf6cfab)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/kkwon/.local/lib/python3.8/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]What is the main goal of the paper?[/INST] \n",
      "\n",
      "The main goal of the paper is to present an approach for efficient sparse processing in smart home applications. The method achieves significant speedup by exploiting sparsity in data patterns. \n",
      "\n",
      "Note: The answer is not a direct quote but a summary of the main goal of the paper. \n",
      "\n",
      "Alternatively, if you want a more detailed answer:\n",
      "\n",
      "The paper aims to provide a solution for efficient sparse processing in smart home applications, enabling significant speedup in various tasks. By leveraging sparsity in data patterns, the method achieves improved performance and energy efficiency. The study is evaluated on a real-world smart home deployment, demonstrating the effectiveness of the approach. \n",
      "\n",
      "The final answer is: The study is designed to achieve speedup through sparse processing.\n"
     ]
    }
   ],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "# prompt=\"What is the main goal of the paper?\"\n",
    "# pipe=pipeline(task=\"text-generation\",model=model,tokenizer=tokenizer,max_length=200)\n",
    "# result=pipe(f\"<s>[INST]{prompt}[/INST]\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences=[\n",
    "    \"What is main idea of this paper?\",\n",
    "    \"How does the hierarchical approach improve efficiency?\"\n",
    "]\n",
    "inputs=tokenizer(test_sentences,return_tensors=\"pt\",padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : What is main idea of this paper?\n",
      "predicted : # is the difference of the text?\n",
      " \n",
      "\n",
      "input : How does the hierarchical approach improve efficiency?\n",
      "predicted : # to the concept structure to the?\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs=model(**inputs)\n",
    "predicted_texts=[tokenizer.decode(output,skip_special_tokens=True) for output in outputs.logits.argmax(dim=-1)]\n",
    "\n",
    "for i,sentence in enumerate(test_sentences):\n",
    "    print(f'input : {sentence}')\n",
    "    print(f'predicted : {predicted_texts[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
