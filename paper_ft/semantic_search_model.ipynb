{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkwon/.local/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/kkwon/.local/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/kkwon/.local/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b5b4efa591492c8e70f62461dc0f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098cb8ef7af3413b9b9e7af136058553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a46ddb5ce9c4c57a817c62802b42b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5050b5344cd94bae972a4ef239102cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a1dc3af034765adf2b9fb8d90951e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88167d4a1e92406fb3c013f510cddb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc51ba02cea4a839e45953c19d544bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ecc57e0e8f41188b6c7085f509ffca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c223699ae534e1fac2b64a0d537ed41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d3f62d9434455eb3727c1215beb473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fcce05f3bf43628730b4eb282debe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad54f1d7fc3f47309eabea5e7197189b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "encoded_model_path = 'semantic_search_model.pt'\n",
    "dataset_path = '/home/kkwon/AHN/paper_ft/cleaned_paper.csv'\n",
    "\n",
    "bi_encoder = SentenceTransformer(model_name)\n",
    "\n",
    "passages = []\n",
    "with open(dataset_path) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        passages.append(row[0])\n",
    "        \n",
    "corpus_embeddings = bi_encoder.encode(\n",
    "    passages, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "torch.save(corpus_embeddings, encoded_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages in PDF: 6\n",
      "Page 1: 6171 characters extracted\n",
      "Page 2: 5578 characters extracted\n",
      "Page 3: 4214 characters extracted\n",
      "Page 4: 6023 characters extracted\n",
      "Page 5: 2765 characters extracted\n",
      "Page 6: 5628 characters extracted\n",
      "Text extraction complete. Total characters extracted: 29507\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def pdf_to_text(pdf_path, skip_start_pages=0, skip_last_pages=0, header_lines=1, footer_lines=1):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"Total pages in PDF: {num_pages}\")\n",
    "\n",
    "        # Adjust the range to process the correct pages\n",
    "        start_page = skip_start_pages\n",
    "        end_page = num_pages - skip_last_pages\n",
    "\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            \n",
    "            if page_text:\n",
    "                print(f\"Page {page_num + 1}: {len(page_text)} characters extracted\")\n",
    "                lines = page_text.splitlines(True)[header_lines:-footer_lines]\n",
    "                text += \"\".join(lines)\n",
    "            else:\n",
    "                print(f\"Page {page_num + 1} is empty or could not be read\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "# Define the input and output file paths\n",
    "pdf_file_path = \"/home/kkwon/AHN/paper_ft/datas/3362743.3362963.pdf\"\n",
    "output_file_path = \"/home/kkwon/AHN/paper_ft/datas/paper1.txt\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "# Adjust skip_start_pages and skip_last_pages as needed\n",
    "raw_text = pdf_to_text(pdf_file_path, skip_start_pages=0, skip_last_pages=0, header_lines=2, footer_lines=1)\n",
    "\n",
    "# Save the extracted text to a text file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(raw_text)\n",
    "\n",
    "print(f\"Text extraction complete. Total characters extracted: {len(raw_text)}\")\n",
    "\n",
    "    \n",
    "data=re.sub(r'[\\n\\t\\r]',' ',raw_text)\n",
    "sentences=re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s',data)\n",
    "sentences=[sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "unique_sentences=list(dict.fromkeys(sentences))\n",
    "\n",
    "df=pd.DataFrame(unique_sentences,columns=['Text'])\n",
    "df.to_csv('/home/kkwon/AHN/paper_ft/cleaned_paper.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'search_results': [{'score': 0.5700194835662842, 'text': 'We test our implementation on two embedded devices which are representative of typical IoT devices you might see in use: Raspberry Pi Zero[ 5], Photon[ 9].'}, {'score': 0.516374945640564, 'text': 'Ideally, our hier- archical models would run on these native computing platforms.'}, {'score': 0.5103737115859985, 'text': 'However they evaluate their systems on relatively high powered devices which aren’t represen- tative of typical edge devices which have fewer resources.'}]}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "bi_encoder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "semantic_search_model = torch.load('semantic_search_model.pt')\n",
    "\n",
    "passages = []\n",
    "with open('/home/kkwon/AHN/paper_ft/cleaned_paper.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        passages.append(row[0])\n",
    "\n",
    "question_embedding = bi_encoder.encode(\n",
    "    \"What hardware platforms were used for testing?\", convert_to_tensor=True)\n",
    "hits = util.semantic_search(\n",
    "    question_embedding, semantic_search_model, top_k=3)\n",
    "hits = hits[0]\n",
    "\n",
    "result = {\"search_results\": [\n",
    "    {\"score\": hit['score'], \"text\": passages[hit['corpus_id']]} for hit in hits]}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
